# -*- coding: utf-8 -*-
"""step2_train_global_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H20fh_MZM_Nm0p1-0dLvUyKglV7CVndP
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset

# -------------------------------------------------
# GLOBAL 1D-CNN AUTOENCODER (HIGH-ACCURACY)
# -------------------------------------------------
class GlobalCNN_Autoencoder(nn.Module):
    def __init__(self, input_features):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),

            nn.Conv1d(32, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),

            nn.MaxPool1d(2)
        )

        self.decoder = nn.Sequential(
            nn.Upsample(scale_factor=2),

            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),

            nn.Conv1d(32, 1, kernel_size=3, padding=1)
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))


# -------------------------------------------------
# DATA LOADER (BENIGN ONLY)
# -------------------------------------------------
def load_benign_data(csv_path, max_samples=200000):
    print(f"Loading benign data from: {csv_path}")

    df = pd.read_csv(csv_path, nrows=max_samples)

    # Keep only numeric features
    df = df.select_dtypes(include=[np.number])

    # Handle invalid values
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna()

    print(f"Cleaned benign samples: {df.shape[0]}")

    # Normalize features
    scaler = StandardScaler()
    X = scaler.fit_transform(df.values)

    return X


# -------------------------------------------------
# TRAINING FUNCTION
# -------------------------------------------------
def train_model(model, data, epochs=30, batch_size=128):
    dataset = TensorDataset(
        torch.tensor(data, dtype=torch.float32).unsqueeze(1)
    )

    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()

    for epoch in range(epochs):
        total_loss = 0.0

        for (x,) in loader:
            optimizer.zero_grad()
            output = model(x)
            loss = loss_fn(output, x)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch [{epoch+1}/{epochs}] | Loss: {total_loss/len(loader):.6f}")


# -------------------------------------------------
# MAIN
# -------------------------------------------------
if __name__ == "__main__":
    # ðŸ”´ CHANGE PATH IF USING COLAB
    CSV_PATH = "/content/Monday-WorkingHours.pcap_ISCX.csv"
    # For local use:
    # CSV_PATH = "data/Monday-WorkingHours.pcap_ISCX.csv"

    benign_data = load_benign_data(CSV_PATH)

    input_features = benign_data.shape[1]
    model = GlobalCNN_Autoencoder(input_features)

    print("\nTraining global CNN autoencoder...")
    train_model(model, benign_data)

    torch.save(
        model.state_dict(),
        "global_cnn_autoencoder.pt"
    )

    print("\nâœ… Global CNN autoencoder trained and saved")